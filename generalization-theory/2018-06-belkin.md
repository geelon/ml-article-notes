---
title: To Understand Deep Learning We Need to Understand Kernel Learning
date: 2018-08-04
---

Mikhail Belkin, Siyuan Ma, and Soumik Mandal. "To understand deep learning we need to understand kernel learning." 2018.

## Summary

Let $\mathcal{H}$ be the RKHS of a kernel $K$. Let $f \in \mathcal{H}$
be any function that achieves perfect (binary) classification with
margin $t$. Then, with high probability, the norm $||f||_\mathcal{H} =
\Omega\left(\mathrm{exp}(n^{1/d})\right)$ with high probability. On the other
hand, most generalization bounds on kernel methods are on the order of
$O\left(\frac{||f||_\mathcal{H}^\alpha}{n^\beta}\right)$. Thus, these
bounds become trivial as $n$ becomes large.

Still, empirically, a function that interpolates the dataset tend to
perform near optimal. 

### Setup

Let $K(\mathbf{x},\mathbf{z}) : \mathbb{R}^d \times \mathbb{R}^d \to
\mathbb{R}$ be a positive definite kernel. Let $\mathcal{H}$ be the
corresponding RKHS. Given a dataset $\{(\mathbf{x}_i, y_i), i =
1,\dotsc, n\}$, let $K$ be the associated kernel matrix $K_{ij} =
K(\mathbf{x}_i, \mathbf{x}_j)$. Define $f^*$ to be the *minimum norm
interpolant*,
$$f^* = \mathrm{arg\ min}_{f \in \mathcal{H}, f(\mathbf{x}_i) = y_i}
||f||_\mathcal{H}.$$
The classical representer theorem implies that $f^*$ exists (assuming
$\mathbf{x}_i \ne \mathbf{x}_j$ when $i \ne j$). In particular,
$$\begin{equation*}
f^* = \sum \alpha_i^* K(\mathbf{x}_i, \cdot),
\end{equation*}$$
where $(\alpha_1^*, \dotsc, \alpha_n^*) = K^{-1}(y_1,\dotsc, y_n)^T$.

The function $f^*$ also minimizes $\sum \ell(f(\mathbf{x}_i), y_i)$
for any nonnegative loss function $\ell(\tilde{y},y)$ such that
$\ell(y,y) = 0$. If $\ell$ is strictly convex, then
$\mathbf{\alpha}^*$ is the unique vector satisfying
$$\mathbf{\alpha}^* = \mathrm{arg\ min}_{\alpha \in \mathbb{R}^n}
\sum_{i=1}^n \ell\left(\left(\sum_{j=1}^n \alpha_i K(\mathbf{x}_j,
\mathbf{x}_i)\right), y_i\right).$$
Note that this allows for iterative methods to solve for
$\mathbf{\alpha}^*$, whereas a matrix inverse requres $n^3$
operations.

If $f(\cdot) = \sum \alpha_i K(\mathbf{x}_i, \cdot)$ is any function
in the RKHS, its norm is:
$$\begin{equation*}
||f||_H^2 = \langle \mathbf{\alpha}, K \mathbf{\alpha}\rangle.
\end{equation*}$$
Two kernels to consider are the smooth Gaussian kernel
$K(\mathbf{x},\mathbf{z}) = \mathrm{exp}\left(-\frac{||\mathbf{x} -
\mathbf{z}||^2}{2\sigma^2}\right)$ and the nonsmooth Laplacian
(exponential) kernel, $K(\mathbf{x}, \mathbf{z}) = \mathrm{exp}\left(-
\frac{||\mathbf{x} - \mathbf{z}||}{\sigma}\right)$.

### Current Bounds

Let $(\mathbf{x}_i, y_i) \in \Omega \times \{-1,1\}$ be a labeled
dataset where $\Omega \subset \mathbb{R}^d$ is a bounded domain. Let
$P$ be a probability measure on $\Omega \times \{-1,1\}$, where the
Bayes classifier does not achieve zero loss.

**Definition 1.** A function $h \in \mathcal{H}$ $t$-*overfits* the
  data if it achieves zero classification loss and for a fixed portion
  of the dataset, $y_ih(\mathbf{x}_i) > t > 0$.

Note that this margin condition is weaker than the interpolating
condition, where we require $h(\mathbf{x}_i) = y_i$.

In the following theorem, we see that if a function $f \in
\mathcal{H}$ $t$-overfits the data (assuming that Bayes optimal
classifier has nonzero loss), then $||f||_\mathcal{H} =
\Omega(\mathrm{exp}(n^{1/d}))$ with high probability.

This shows that as $n$ grows, the norm of $||f||$ must grow
exponentially. However, generalization bounds tend to be polynomial in
$n$, and so the bounds diverge to infinity. Still, empirically,
performance of interpolated classifiers remain near optimal, even with
label noise.

**Theorem 2.** Let $(\mathbf{x}_i, y_i)$ be data sampled from $P$ (we
  assume that $y$ is not a deterministic function of $x$ on a subset
  of non-zero measure). Then, with high probability, any $h$ that
  $t$-overfits the data satisfies:
  $$||h||_{\mathcal{H}} > A e^{B n^{1/d}},$$
  for constants $A,B > 0$ depending on $t$.

*Proof.* The goal will be to show that the open ball $B_R \subset
 \mathcal{H}$ of radius $R$ contains no functions that $t$-overfit the
 data with high probability. The proof uses a classical result on
 fat-shattering dimension: let $V_\gamma(B_R)$ be the fat-shattering
 dimension of $B_R$ with parameter $\gamma$ and let $\ell$ be the
 hinge loss with margin $t$. Then, there exists $C_1, C_2 > 0$ such
 that with high probability, for all $f \in B_R$,
 $$\left|\frac{1}{n} \sum_i \ell(f(\mathbf{x}_i, y_i) -
 \mathbb{E}_P[\ell(f(\mathbf{x}), )]\right| \leq C_1 \gamma + C_2
 \sqrt{\frac{V_\gamma(B_R)}{n}}.$$ 

 If $f$ $t$-overfits the data, then the empirical loss is zero. We can
 choose $\gamma$ small enough so that $C_1 \gamma <
 \mathbb{E}_P[\ell(f(\mathbf{x}),y)]$, leaving a lower bound:
 $$\begin{align*}
 \mathbb{E}_P[\ell(f(\mathbf{x}),y)] - C_1 \gamma < C_2
 \sqrt{\frac{V_\gamma(B_R)}{n}}.\end{align*}$$
 In short, this shows that $n < C V_\gamma(B_R)$. However,
 $V_\gamma(B_R) <
 O\left(\log^d\left(\frac{R}{\gamma}\right)\right)$. This implies that
 $R > A e^{B n^{1/d}}$ for some positive constants $A,B > 0$.
<div align="right">‚òê</div>

**Remark 3.** Most bounds on kernel methods are of the form:
$$\left|\frac{1}{n} \sum_i \ell(f(\mathbf{x}),y) -
\mathbb{E}_P[\ell(f(\mathbf{x}), y)]\right| \leq C_1 + C_2
\frac{||f||_\mathcal{H}^\alpha}{n^\beta},$$
for $C_1, C_2, \alpha, \beta \geq 0$. Thus, the superpolynomial bound
on $||f||_\mathcal{H}$ means that these bounds become trivial as $n$
becomes large.


## Discussion

**Note 1.** It is noted that "kernel machines can be viewed as linear
  regression in infinite dimensional RKHS's...they can also be
  interpreted as two-layer neural networks with a fixed first layer."
  Should understand what this means more deeply.

**Note 2.** Their optimization method is the EigenPro-SGD
  method. Should understand what this is.

**Question 3.** Can one apply this method and say something about
  bounds on other (not kernel-based) methods? They do say that it is
  not clear how to construct a norm for deep neural networks analogous
  to RKHS. A recent attempt: (Neyshabur Bhojanapalli McAllester Srebro 2017).

**Note 4.** They mention that they know of only a few non "norm-based
  concentration bounds" for kernel methods. These include one based on
  1-nearest neighbor classification (Cover Hart 1967).

### Further Reading

- [[Cover Hart 1967](https://ieeexplore.ieee.org/document/1053964/)] Nearest neight pattern classification.
- [[Neyshabur Bhojanapalli McAllester Srebro 2017](./)] Exploring generalization in deep learning.