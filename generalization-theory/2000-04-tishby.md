---
title: The Information Bottleneck Method
date: 2018-01-16
---

Tishby, Naftali, Fernando C. Pereira, and William Bialek. "The information bottleneck method." arXiv preprint physics/0004057 (2000).

## Summary

Many learning problem have the same structure: "extract the
information from one variable $X$ that is relevant for the prediction
of another one, $Y$." This problem generalizes one in *rate-distortion
theory*: let $\tilde{X}$ encode the signal $X$. What is, on average,
the minimal bits per signal (the *rate* $R$) needed so that the
reconstruction of the signal has distortion at most $D$? 

This latter question is characterized by the *rate-distortion
function*, $R(D)$, where the distortion is determined by some
*distortion function*, $d : X \times \tilde{X} \to \mathbb{R}^+$. 

In learning, instead of reconstructing $X$ from $\tilde{X}$, we
attempt to determine $Y$ from $\tilde{X}$. Still, it is not clear what 
the correct choice of distortion function is; indeed, if we associate
each distortion function to a maximally "relevant" feature, this is
equivalent to a feature selection.

### Rate-distortion theory

We think of the encoding or *quantization* of $X$ to $\tilde{X}$ as a
stochastic function, given as a distribution by $p(\tilde{x} |
x)$. Note that:

- given $\tilde{X}$, on average, we need $H(X|\tilde{X})$ many bits to
specify $X$. So, on average, $2^{H(X|\tilde{X})}$ sources are encoded
with the same code;
- it follows that there are $2^{H(X)}/2^{H(X|\tilde{X})} =
2^{I(X;\tilde{X})}$ codes. Thus the minimal number of bits per symbol
(i.e. rate) is bounded below by $I(X;\tilde{X})$ (i.e. information
rate).  









## Discussion

### Keywords

- rate-distortion theory, see [Cover 1991](https://s3.amazonaws.com/academia.edu.documents/31823797/information_theory.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1516153305&Signature=40bNJTWPBI%2BdGhR8SVX8Xoq7LFQ%3D&response-content-disposition=inline%3B%20filename%3DElements_of_Information_Theory_Elements.pdf)