---
title: ML Articles Reading Notes
---

# ML Articles Reading Notes

Notes in chronological order: [archive](https://geelon.github.io/thesis-notes.html)

## Fundamentals

- [Shalev-Shwartz 2014](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf), Understanding Machine Learning: from Theory to Algorithms
    - Chapter 4: Learning via Uniform Convergence, [notes](./fundamentals/2014-UML-chapter-4.md)
    - Chapter 5: No Free Lunch and Error Decomposition, [notes](./fundamentals/2014-UML-chapter-5.md)
    - Chapter 6: VC Dimension, [notes](./fundamentals/2014-UML-chapter-6.md)
    - Chapter 7: Nonuniform Learnability and SRM, [notes](./fundamentals/2014-UML-chapter-5.md)
    
## Evolution Strategies (Derivative-Free Optimization)

- [Salimans 2017](https://arxiv.org/pdf/1703.03864.pdf): Evolution Strategies as a Scalable Alternative to Reinforcement Learning, [notes](./evolution-strategies/2017-09-salimans.md)
- [Lehman 2017](https://arxiv.org/pdf/1712.06568.pdf): ES Is More Than Just a Traditional Finite-Difference Approximator, [notes](./evolution-strategies/2017-12-lehman.md)
- [Zhang 2017](https://arxiv.org/pdf/1712.06564.pdf): On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent, [notes](./evolution-strategies/2017-12-zhang.md)

## Generalization Theory
- [Tishby 2000](https://arxiv.org/pdf/physics/0004057.pdf): The Information Bottleneck Method, [notes](./generalization-theory/2000-04-tishby.md)
- [Tishby 2015](https://arxiv.org/pdf/1503.02406.pdf): Deep Learning and the Information Bottleneck Principle, [notes](./generalization-theory/2015-03-tishby.md)