---
title: ES Is More Than Just a Traditional Finite-Difference Approximator
date: 2018-01-11
---

Lehman, Joel, et al. "ES Is More Than Just a Traditional Finite-Difference Approximator." arXiv preprint arXiv:1712.06568 (2017).

## Summary

First, the *finite difference* method estimates the gradient by
approximating the derivative by small perturbations. For example, in
one dimension, we can estimate $f'(x)$ by:
$$f'(x) \approx \frac{f(x + \delta) - f(x)}{\delta}.$$
However, this method scales poorly when we extend it to higher
dimensions, if we perturb each parameter individually. Furthermore,
this method is designed to estimate the gradient of a function $F$ at
a single point $\theta$ in the parameter space. This allows a learner
to optimize an objective function $F$ using gradient techniques, even
if $F$ is not differentiable.

In contrast, the goal of *ES* is to optimize the objective function:
$$J(\theta) = \mathbb{E}_{z \sim p_\theta} F(z) = \int f(z)
\pi(z|\theta) dz.$$
And so, the difference is that ES optimizes *distributional
parameters*. Indeed, the learner is not looking for the 'fittest'
individual point in parameter space, but the fittest population
(defined by the distribution parameters $\theta$).

In ES, the population distribution is an isotropic Gaussian, with
covariance $\sigma^2 I$. Here, $\sigma$ controls the *robustness* of
the population genotype: a smaller $\sigma$ corresponds with smaller
perturbations, and if $F$ is continuous (almost everywhere), then this
also implies smaller 'genotype variation' across the population.

The *robustness* of the population genotype is linked with
*evolvability*. If the population genotype is not robust, then it is
not very meaningful beyond being the mean of individual genotypes; its
evolution might also not mean very much.

However, allowing the learning mechanism to adjust robustness may lead
to prioritizing robustness over evolvability (e.g. trivially, the
Dirac delta distribution is completely robust, but loses all
advantages of optimizing population distributions).

Conversely, if $\sigma$ is large and ES converges, then it finds a
region of the search space with robust parameters: the behavior of the
objective function doesn't change much even with relatively large
perturbations of the parameters.

### Empirical Results

This paper also tested ES on different 'fitness landscapes' to compare
the behavior of ES with finite-difference gradient descent. One
landscape that ES had trouble with was the 'narrowing path landscape',
where the path to the global minimum had increasingly steep walls in
one direction---ultimately ES demands too much robustness to move
further along the path:

<video autoplay controls controlslist="nodownload" loop preload="metadata" width="480" height="270" class="arve-video fitvidsignore"><source type="video/mp4" src="https://eng.uber.com/wp-content/uploads/2017/12/narrowing_path_composite.mp4"></video>

## Discussion

**Question 1:** It seems that if the objective function were Lipschitz
  (possibly a.e.), then ES would always converge to a local
  minimum provided that $\sigma$ is small enough. Can we prove this?

**Question 2:** Let $\mathcal{G}_\sigma(F)$ be the new objective function
  constructed from $F$ (i.e. $\mathcal{G}(F) = J$, in the notation
  above). What is the behavior of if $\mathcal{G}_\sigma$ is applied
  to $F$ iteratively? Is there a fixed point? What if $\sigma$ is
  varied continuously? Is $\mathcal{G}_\sigma^2 =
  \mathcal{G}_{2\sigma}$ (unlikely, but are there such algebraic
  properties)?

**Note 3:** Different landscapes have different ideal covariances. One
  natural generalization is to allow the covariance $\Sigma$ of the
  population parameters to be diagonal (and not just $\sigma^2
  I$). One can think of this as a rescaling of the parameter space.

**Aside 4:** It seems that the language of 'genotype', 'robustness',
  'population' may be replaced with more topological language. The
  population's parameter distribution as a proxy for an open set (or
  in the case of ES, an open ball) on the parameter space, or vice
  versa. 