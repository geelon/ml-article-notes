---
title: The Information Bottleneck Method
date: 2018-01-16
---

Tishby, Naftali, Fernando C. Pereira, and William Bialek. "The information bottleneck method." arXiv preprint physics/0004057 (2000).

## Summary

Many learning problems have the same structure: "extract the
information from one variable that is relevant for the prediction
of another one." This problem generalizes one in *rate-distortion
theory*: let $\tilde{X}$ encode the signal $X$. From $\tilde{X}$, how
well can we predict $X$? In particular, what is, on average, the
minimal bits per signal (the *rate* $R$) needed so that the 
reconstruction of the signal has distortion at most $D$? 

This latter question is characterized by the *rate-distortion
function*, $R(D)$, where the distortion is measured by some
*distortion function*, $d : X \times \tilde{X} \to \mathbb{R}^+$. 

In learning, instead of reconstructing $X$ from $\tilde{X}$, we
attempt to determine some $Y$ from $\tilde{X}$. Still, it is not clear
what the correct choice of distortion function is; indeed, by
associating each distortion function to a maximally "relevant" feature,
we see that choosing a distortion function is equivalent to selecting
a feature.

### Rate-distortion theory

We let the encoding or *quantization* of $X$ as $\tilde{X}$ be a
stochastic function, given as a distribution by $p(\tilde{x} |
x)$. Note that:

- given $\tilde{X}$, on average, we need $H(X|\tilde{X})$ many bits to
specify $X$. So, on average, $2^{H(X|\tilde{X})}$ sources are encoded
with the same code;
- it follows that there are $2^{H(X)}/2^{H(X|\tilde{X})} =
2^{I(X;\tilde{X})}$ codes. Thus the minimal number of bits per symbol
(i.e. rate) is bounded below by $I(X;\tilde{X})$ (i.e. information
rate).  

We want to minimize rate while being constrained by a maximal
distortion. That is, we define the *rate-distortion function* as:
$$R(D) \equiv \inf_{p(\tilde{x}|x) :
\mathbb{E}_p[d(x,\tilde{x})]\leq D} I(X;\tilde{X}),$$
solved by minimizing the functional,
$$\mathcal{F}\big[p(\tilde{x}|x)\big] = I(X;\tilde{X}) + \beta
\mathbb{E}_{p(\tilde{x}|x)} d(x,\tilde{x}).$$
This is minimized at:
$$p(\tilde{x}|x) = \frac{p(\tilde{x})}{Z(x,\beta)} \exp\left\{-\beta d(x,\tilde{x})\right\},$$
where $Z(x,\beta)$ is a normalization function. Because the
probability distribution must also satisfy $p(\tilde{x}) = \sum
p(x)p(\tilde{x}|x)$, this gives way to the *Blahut-Arimoto algorithm*
to solve for $p(\tilde{x}|x)$.

### Generalization to Learning

In learning, the goal is to then compress the information source $X$
into $\tilde{X}$ while preserving as much information about a relevant
signal $Y$. That is, passing information through a "bottleneck"
provided by $Y$ (thus the *information bottleneck* method). And so,
instead of minimizing distortion, we want to maximize
$I(\tilde{X};Y)$. This yields the functional: 
$$\mathcal{L}\big[p(\tilde{x}|x)\big] = I(\tilde{X};X) - \beta
I(\tilde{X};Y).$$
Note that of course, $I(\tilde{X};Y) \leq I(X;Y)$.

It turns out that the formal solution is:
$$p(\tilde{x}|x) = \frac{p(\tilde{x})}{Z(x,\beta)} \exp\bigg\{-\beta
 \ D_{KL}\big[\,p(y|x) \ \big|\big|\ p(y|\tilde{x})\,\big]\bigg\}$$
(Formal because $p(y|\tilde{x})$ is defined implicitly by
$p(\tilde{x}|x)$. See paper for derivation. This implies that the
KL-divergence of $p(y|x)$ and $p(y|\tilde{x})$ is the natural
distortion $d(x,\tilde{x})$ for the information bottleneck method. And
as before, this leads to an iterative method obtain the minimizer. (??
might be worth understanding this better ??)

### Structure of Solution

Let $I_X \equiv I(X;\tilde{X})$ and $I_Y \equiv
I(Y;\tilde{X})$. Fixing the cardinality of $\tilde{X}$, the choice of
$\beta$ produces a minimizer to $\mathcal{L}$ from above. The
corresponding values for $I_X$ and $I_Y$ is a point on this
"information plane", and together, forms a curve beginning at $(0,0)$
(corresponding to the minimizer at $\beta = 0$: map all of $X$ to a
single symbol so that $I(X;\tilde{X}) = 0$). This curve is infinitely
steep at the origin.

And so, consider the family of such curves (each corresponding to the
choice of cardinality of $\tilde{X}$). Each two curves separate or
*bifurcate* at some finite *critical* $\beta$ "through a second order
phase transition". More is described in [Chechik 2003](http://papers.nips.cc/paper/2223-extracting-relevant-structures-with-side-information.pdf),
[Slonim 1999](http://papers.nips.cc/paper/1651-agglomerative-information-bottleneck.pdf),
and [Pereira 1993](https://arxiv.org/pdf/cmp-lg/9408011.pdf).

## Discussion

**Note 1:** Seems worthwhile to understand the variational calculus,
  rate-distortion theory, and deterministic annealing better. Also,
  this last note on the "structure of the solution"...more insight on
  the bifurcation?

### Keywords/Further Reading

- rate-distortion theory, see [Cover 1991](https://s3.amazonaws.com/academia.edu.documents/31823797/information_theory.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1516153305&Signature=40bNJTWPBI%2BdGhR8SVX8Xoq7LFQ%3D&response-content-disposition=inline%3B%20filename%3DElements_of_Information_Theory_Elements.pdf)
- calculus of variations, see [Figueroa-O'Farrill](http://www.maths.ed.ac.uk/~jmf/Teaching/Lectures/CoV.pdf)
- Blahut-Arimoto algorithm
- deterministic annealing approach (?? paper said that this is implied
 ??), see also [Ueda 1998](https://papers.nips.cc/paper/941-deterministic-annealing-variant-of-the-em-algorithm.pdf)
- [Slonim 1999](http://papers.nips.cc/paper/1651-agglomerative-information-bottleneck.pdf), Agglomerative information bottleneck
- [Chechik 2003](http://papers.nips.cc/paper/2223-extracting-relevant-structures-with-side-information.pdf), Extracting relevant structures with side information
- [Pereira 1993](https://arxiv.org/pdf/cmp-lg/9408011.pdf),
 Distributional clustering of English words
