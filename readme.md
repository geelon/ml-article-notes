---
title: ML Articles Reading Notes
---

# ML Articles Reading Notes*

Notes in chronological order: [archive](https://geelon.github.io/thesis-notes.html)



## Fundamentals
- [Shalev-Shwartz 2014](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf), Understanding Machine Learning: from Theory to Algorithms
    - Chapter 3: PAC learning, [notes](./fundamentals/2014-UML-chapter-3.md)
    - Chapter 4: Learning via Uniform Convergence, [notes](./fundamentals/2014-UML-chapter-4.md)
    - Chapter 5: No Free Lunch and Error Decomposition, [notes](./fundamentals/2014-UML-chapter-5.md)
    - Chapter 6: VC Dimension, [notes](./fundamentals/2014-UML-chapter-6.md)
    - Chapter 7: Nonuniform Learnability and SRM, [notes](./fundamentals/2014-UML-chapter-7.md)
- [Cucker Zhou 2007](http://www.cambridge.org/gb/academic/subjects/computer-science/pattern-recognition-and-machine-learning/learning-theory-approximation-theory-viewpoint), Learning Theory: an approximation viewpoint
    - Chapter 1: The Framework of Learning, [notes](./fundamentals/2007-LT-chapter-1.md)
    - Chapter 2: Basic Hypothesis Spaces, [notes](./fundamentals/2007-LT-chapter-2.md)
- [Murphy 2012](./), Machine Learning: a probabilistic approach
    - Chapter 3: Generative Models for Discrete Data, [notes](./fundamentals/2012-MLPA-chapter-3.md)
    - Chapter 10: Directed Graphical Models, [notes](./fundamentals/2012-MLPA-chapter-10.md)


## Statistics
- [Minsker 2015](https://arxiv.org/abs/1308.1334): Geometric median and robust estimation in Banach spaces, [notes](./statistics/2015-geometric-median.md)
    

## Generalization Theory
- [Belkin 2018](./): To Understand Deep Learning We Need to Understand Kernel Learning, [notes](./generalization-theory/2018-06-belkin.md)
- [Tishby 2000](https://arxiv.org/pdf/physics/0004057.pdf): The Information Bottleneck Method, [notes](./generalization-theory/2000-04-tishby.md)
- [Tishby 2015](https://arxiv.org/pdf/1503.02406.pdf): Deep Learning and the Information Bottleneck Principle, [notes](./generalization-theory/2015-03-tishby.md)


## Transfer Learning
- [Pan Yang 2010](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf): A Survey on Transfer Learning, [notes](./transfer/2010-07-pan-yang.md)

## Unsupervised Learning
- [Yu Mineyev Varshney 2018](https://arxiv.org/pdf/1807.11167v1.pdf): A Group-Theoretic Approach to Abstraction, [notes](./unsupervised/2018-07-group-theoretic-approach-to-abstraction.md)


## Optimization
- [Jain Kar 2017](https://arxiv.org/pdf/1712.07897.pdf), Non-convex Optimization for Machine Learning
     - Chapter 1: Introduction, [notes](./optimization/2017-nonconvex-chapter-1.md)
     - Chapter 2: Mathematical Tools, [notes](./optimization/2017-nonconvex-chapter-2.md)
     - Chapter 4: Alternating Minimization, [notes](./optimization/2017-nonconvex-chapter-4.md)
     - Chapter 5: EM Algorithm, [notes](./optimization/2017-nonconvex-chapter-5.md)
- [Salimans 2017](https://arxiv.org/pdf/1703.03864.pdf): Evolution Strategies as a Scalable Alternative to Reinforcement Learning, [notes](./optimization/2017-09-salimans.md)
- [Lehman 2017](https://arxiv.org/pdf/1712.06568.pdf): ES Is More Than Just a Traditional Finite-Difference Approximator, [notes](./optimization/2017-12-lehman.md)
- [Zhang 2017](https://arxiv.org/pdf/1712.06564.pdf): On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent, [notes](./optimization/2017-12-zhang.md)


<em>*The phrasing in these notes are sometimes copied directly from the texts. Other times, the notes diverge quite a bit.</em>